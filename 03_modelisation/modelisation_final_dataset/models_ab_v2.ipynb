{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>week</th>\n",
       "      <th>awayteam</th>\n",
       "      <th>hometeam</th>\n",
       "      <th>idgame</th>\n",
       "      <th>winner_home</th>\n",
       "      <th>home_coach</th>\n",
       "      <th>away_coach</th>\n",
       "      <th>weather_type</th>\n",
       "      <th>temperature</th>\n",
       "      <th>...</th>\n",
       "      <th>punt_return_yds_MA_5_home</th>\n",
       "      <th>kicking_pts_MA_5_home</th>\n",
       "      <th>delta_day_away</th>\n",
       "      <th>delta_day_home</th>\n",
       "      <th>stade</th>\n",
       "      <th>location</th>\n",
       "      <th>people</th>\n",
       "      <th>month</th>\n",
       "      <th>dayofmonth</th>\n",
       "      <th>dayofweek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>13</td>\n",
       "      <td>Vikings</td>\n",
       "      <td>Falcons</td>\n",
       "      <td>400951677</td>\n",
       "      <td>0</td>\n",
       "      <td>Dan Quinn</td>\n",
       "      <td>Mike Zimmer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>10.4</td>\n",
       "      <td>9.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Mercedes-Benz Stadium</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>95.0</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>9</td>\n",
       "      <td>Falcons</td>\n",
       "      <td>Panthers</td>\n",
       "      <td>400951749</td>\n",
       "      <td>1</td>\n",
       "      <td>Ron Rivera</td>\n",
       "      <td>Dan Quinn</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>61.0</td>\n",
       "      <td>...</td>\n",
       "      <td>22.4</td>\n",
       "      <td>7.4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Bank of America Stadium</td>\n",
       "      <td>Charlotte, NC</td>\n",
       "      <td>100.0</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>9</td>\n",
       "      <td>Bengals</td>\n",
       "      <td>Jaguars</td>\n",
       "      <td>400951753</td>\n",
       "      <td>1</td>\n",
       "      <td>Doug Marrone</td>\n",
       "      <td>Marvin Lewis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>8.4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>TIAA Bank Field</td>\n",
       "      <td>Jacksonville, FL</td>\n",
       "      <td>89.0</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017</td>\n",
       "      <td>4</td>\n",
       "      <td>Bears</td>\n",
       "      <td>Packers</td>\n",
       "      <td>400951678</td>\n",
       "      <td>1</td>\n",
       "      <td>Mike McCarthy</td>\n",
       "      <td>John Fox</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>65.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Lambeau Field</td>\n",
       "      <td>Green Bay, WI</td>\n",
       "      <td>97.0</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017</td>\n",
       "      <td>9</td>\n",
       "      <td>Colts</td>\n",
       "      <td>Texans</td>\n",
       "      <td>400951751</td>\n",
       "      <td>0</td>\n",
       "      <td>Bill O'Brien</td>\n",
       "      <td>Chuck Pagano</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>84.0</td>\n",
       "      <td>...</td>\n",
       "      <td>31.4</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NRG Stadium</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>100.0</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   season  week awayteam  hometeam     idgame  winner_home     home_coach  \\\n",
       "0    2017    13  Vikings   Falcons  400951677            0      Dan Quinn   \n",
       "1    2017     9  Falcons  Panthers  400951749            1     Ron Rivera   \n",
       "2    2017     9  Bengals   Jaguars  400951753            1   Doug Marrone   \n",
       "3    2017     4    Bears   Packers  400951678            1  Mike McCarthy   \n",
       "4    2017     9    Colts    Texans  400951751            0   Bill O'Brien   \n",
       "\n",
       "     away_coach weather_type  temperature  ...  punt_return_yds_MA_5_home  \\\n",
       "0   Mike Zimmer          NaN          NaN  ...                       10.4   \n",
       "1     Dan Quinn       cloudy         61.0  ...                       22.4   \n",
       "2  Marvin Lewis          NaN         78.0  ...                        0.4   \n",
       "3      John Fox       cloudy         65.0  ...                        NaN   \n",
       "4  Chuck Pagano       cloudy         84.0  ...                       31.4   \n",
       "\n",
       "   kicking_pts_MA_5_home  delta_day_away  delta_day_home  \\\n",
       "0                    9.4            10.0             7.0   \n",
       "1                    7.4             7.0             7.0   \n",
       "2                    8.4             7.0            14.0   \n",
       "3                    NaN             4.0             4.0   \n",
       "4                    9.0             7.0             6.0   \n",
       "\n",
       "                     stade          location  people  month  dayofmonth  \\\n",
       "0    Mercedes-Benz Stadium       Atlanta, GA    95.0     12           3   \n",
       "1  Bank of America Stadium     Charlotte, NC   100.0     11           5   \n",
       "2          TIAA Bank Field  Jacksonville, FL    89.0     11           5   \n",
       "3            Lambeau Field     Green Bay, WI    97.0      9          29   \n",
       "4              NRG Stadium       Houston, TX   100.0     11           5   \n",
       "\n",
       "   dayofweek  \n",
       "0          6  \n",
       "1          6  \n",
       "2          6  \n",
       "3          4  \n",
       "4          6  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../04_datasets/nfl_dataset_vf.csv\", index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['season', 'week', 'awayteam', 'hometeam', 'idgame', 'winner_home', 'home_coach', 'away_coach', 'weather_type', 'temperature', 'humidity', 'wind', 'streak_away', 'pass_yds_MA_5_away', 'pass_td_MA_5_away', 'rush_yds_MA_5_away', 'rush_td_MA_5_away', 'rec_yds_MA_5_away', 'rec_td_MA_5_away', 'fumbles_MA_5_away', 'fumbles_rec_MA_5_away', 'defense_sacks_MA_5_away', 'defense_td_MA_5_away', 'interceptions_MA_5_away', 'interceptions_td_MA_5_away', 'kicks_return_yds_MA_5_away', 'kicks_return_td_MA_5_away', 'punt_return_yds_MA_5_away', 'kicking_pts_MA_5_away', 'streak_home', 'pass_yds_MA_5_home', 'pass_td_MA_5_home', 'rush_yds_MA_5_home', 'rush_td_MA_5_home', 'rec_yds_MA_5_home', 'rec_td_MA_5_home', 'fumbles_MA_5_home', 'fumbles_rec_MA_5_home', 'defense_sacks_MA_5_home', 'defense_td_MA_5_home', 'interceptions_MA_5_home', 'interceptions_td_MA_5_home', 'kicks_return_yds_MA_5_home', 'kicks_return_td_MA_5_home', 'punt_return_yds_MA_5_home', 'kicking_pts_MA_5_home', 'delta_day_away', 'delta_day_home', 'stade', 'location', 'people', 'month', 'dayofmonth', 'dayofweek']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First: some preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['season', 'week', 'awayteam', 'hometeam', 'idgame', 'winner_home', 'home_coach', 'away_coach', 'weather_type', 'temperature', 'humidity', 'wind', 'streak_away', 'pass_yds_MA_5_away', 'pass_td_MA_5_away', 'rush_yds_MA_5_away', 'rush_td_MA_5_away', 'fumbles_MA_5_away', 'fumbles_rec_MA_5_away', 'defense_sacks_MA_5_away', 'defense_td_MA_5_away', 'interceptions_MA_5_away', 'interceptions_td_MA_5_away', 'kicks_return_yds_MA_5_away', 'kicks_return_td_MA_5_away', 'punt_return_yds_MA_5_away', 'kicking_pts_MA_5_away', 'streak_home', 'pass_yds_MA_5_home', 'pass_td_MA_5_home', 'rush_yds_MA_5_home', 'rush_td_MA_5_home', 'rec_yds_MA_5_home', 'rec_td_MA_5_home', 'fumbles_MA_5_home', 'fumbles_rec_MA_5_home', 'defense_sacks_MA_5_home', 'defense_td_MA_5_home', 'interceptions_MA_5_home', 'interceptions_td_MA_5_home', 'kicks_return_yds_MA_5_home', 'kicks_return_td_MA_5_home', 'punt_return_yds_MA_5_home', 'kicking_pts_MA_5_home', 'delta_day_away', 'delta_day_home', 'stade', 'location', 'people', 'month', 'dayofmonth', 'dayofweek']\n"
     ]
    }
   ],
   "source": [
    "# removing linked columns \n",
    "linked_columns = ['rec_yds_MA_5_away', 'rec_td_MA_5_away']\n",
    "df = df.drop(columns=linked_columns, axis=1)\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating list of columns to test models\n",
    "team_metrics = ['pass_yds_MA_5_away', 'pass_td_MA_5_away', 'rush_yds_MA_5_away', 'rush_td_MA_5_away', 'fumbles_MA_5_away', 'fumbles_rec_MA_5_away', 'defense_sacks_MA_5_away', \n",
    "                'defense_td_MA_5_away', 'interceptions_MA_5_away', 'interceptions_td_MA_5_away', 'kicks_return_yds_MA_5_away', 'kicks_return_td_MA_5_away', 'punt_return_yds_MA_5_away', \n",
    "                'kicking_pts_MA_5_away','pass_yds_MA_5_home', 'pass_td_MA_5_home', 'rush_yds_MA_5_home', 'rush_td_MA_5_home', 'rec_yds_MA_5_home', 'rec_td_MA_5_home', \n",
    "                'fumbles_MA_5_home', 'fumbles_rec_MA_5_home', 'defense_sacks_MA_5_home', 'defense_td_MA_5_home', 'interceptions_MA_5_home', 'interceptions_td_MA_5_home', 'kicks_return_yds_MA_5_home', \n",
    "                'kicks_return_td_MA_5_home', 'punt_return_yds_MA_5_home', 'kicking_pts_MA_5_home']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_metrics_win = ['winner_home', 'pass_yds_MA_5_away', 'pass_td_MA_5_away', 'rush_yds_MA_5_away', 'rush_td_MA_5_away', 'fumbles_MA_5_away', 'fumbles_rec_MA_5_away', 'defense_sacks_MA_5_away', \n",
    "                'defense_td_MA_5_away', 'interceptions_MA_5_away', 'interceptions_td_MA_5_away', 'kicks_return_yds_MA_5_away', 'kicks_return_td_MA_5_away', 'punt_return_yds_MA_5_away', \n",
    "                'kicking_pts_MA_5_away', 'streak_home', 'pass_yds_MA_5_home', 'pass_td_MA_5_home', 'rush_yds_MA_5_home', 'rush_td_MA_5_home', 'rec_yds_MA_5_home', 'rec_td_MA_5_home', \n",
    "                'fumbles_MA_5_home', 'fumbles_rec_MA_5_home', 'defense_sacks_MA_5_home', 'defense_td_MA_5_home', 'interceptions_MA_5_home', 'interceptions_td_MA_5_home', 'kicks_return_yds_MA_5_home', \n",
    "                'kicks_return_td_MA_5_home', 'punt_return_yds_MA_5_home', 'kicking_pts_MA_5_home']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['season', 'week', 'awayteam', 'hometeam', 'idgame', 'winner_home', 'home_coach', 'away_coach', 'weather_type', 'temperature', 'humidity', 'wind', 'streak_away', 'streak_home', 'delta_day_away', 'delta_day_home', 'stade', 'location', 'people', 'month', 'dayofmonth', 'dayofweek']\n"
     ]
    }
   ],
   "source": [
    "df2 = df.drop(columns=team_metrics, axis=1)\n",
    "print(df2.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>week</th>\n",
       "      <th>awayteam</th>\n",
       "      <th>hometeam</th>\n",
       "      <th>idgame</th>\n",
       "      <th>winner_home</th>\n",
       "      <th>home_coach</th>\n",
       "      <th>away_coach</th>\n",
       "      <th>weather_type</th>\n",
       "      <th>temperature</th>\n",
       "      <th>...</th>\n",
       "      <th>streak_away</th>\n",
       "      <th>streak_home</th>\n",
       "      <th>delta_day_away</th>\n",
       "      <th>delta_day_home</th>\n",
       "      <th>stade</th>\n",
       "      <th>location</th>\n",
       "      <th>people</th>\n",
       "      <th>month</th>\n",
       "      <th>dayofmonth</th>\n",
       "      <th>dayofweek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>13</td>\n",
       "      <td>Vikings</td>\n",
       "      <td>Falcons</td>\n",
       "      <td>400951677</td>\n",
       "      <td>0</td>\n",
       "      <td>Dan Quinn</td>\n",
       "      <td>Mike Zimmer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Mercedes-Benz Stadium</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>95.0</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>9</td>\n",
       "      <td>Falcons</td>\n",
       "      <td>Panthers</td>\n",
       "      <td>400951749</td>\n",
       "      <td>1</td>\n",
       "      <td>Ron Rivera</td>\n",
       "      <td>Dan Quinn</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>61.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Bank of America Stadium</td>\n",
       "      <td>Charlotte, NC</td>\n",
       "      <td>100.0</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>9</td>\n",
       "      <td>Bengals</td>\n",
       "      <td>Jaguars</td>\n",
       "      <td>400951753</td>\n",
       "      <td>1</td>\n",
       "      <td>Doug Marrone</td>\n",
       "      <td>Marvin Lewis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>TIAA Bank Field</td>\n",
       "      <td>Jacksonville, FL</td>\n",
       "      <td>89.0</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017</td>\n",
       "      <td>4</td>\n",
       "      <td>Bears</td>\n",
       "      <td>Packers</td>\n",
       "      <td>400951678</td>\n",
       "      <td>1</td>\n",
       "      <td>Mike McCarthy</td>\n",
       "      <td>John Fox</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>65.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Lambeau Field</td>\n",
       "      <td>Green Bay, WI</td>\n",
       "      <td>97.0</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017</td>\n",
       "      <td>9</td>\n",
       "      <td>Colts</td>\n",
       "      <td>Texans</td>\n",
       "      <td>400951751</td>\n",
       "      <td>0</td>\n",
       "      <td>Bill O'Brien</td>\n",
       "      <td>Chuck Pagano</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>84.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NRG Stadium</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>100.0</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   season  week awayteam  hometeam     idgame  winner_home     home_coach  \\\n",
       "0    2017    13  Vikings   Falcons  400951677            0      Dan Quinn   \n",
       "1    2017     9  Falcons  Panthers  400951749            1     Ron Rivera   \n",
       "2    2017     9  Bengals   Jaguars  400951753            1   Doug Marrone   \n",
       "3    2017     4    Bears   Packers  400951678            1  Mike McCarthy   \n",
       "4    2017     9    Colts    Texans  400951751            0   Bill O'Brien   \n",
       "\n",
       "     away_coach weather_type  temperature  ...  streak_away  streak_home  \\\n",
       "0   Mike Zimmer          NaN          NaN  ...          7.0          3.0   \n",
       "1     Dan Quinn       cloudy         61.0  ...          1.0          1.0   \n",
       "2  Marvin Lewis          NaN         78.0  ...          1.0          1.0   \n",
       "3      John Fox       cloudy         65.0  ...          1.0          1.0   \n",
       "4  Chuck Pagano       cloudy         84.0  ...         -3.0         -1.0   \n",
       "\n",
       "   delta_day_away  delta_day_home                    stade          location  \\\n",
       "0            10.0             7.0    Mercedes-Benz Stadium       Atlanta, GA   \n",
       "1             7.0             7.0  Bank of America Stadium     Charlotte, NC   \n",
       "2             7.0            14.0          TIAA Bank Field  Jacksonville, FL   \n",
       "3             4.0             4.0            Lambeau Field     Green Bay, WI   \n",
       "4             7.0             6.0              NRG Stadium       Houston, TX   \n",
       "\n",
       "  people month  dayofmonth  dayofweek  \n",
       "0   95.0    12           3          6  \n",
       "1  100.0    11           5          6  \n",
       "2   89.0    11           5          6  \n",
       "3   97.0     9          29          4  \n",
       "4  100.0    11           5          6  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model with the team ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model hypothesis: all the features, random state at 0, imputer = mean or most frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set :  0.7224137931034482\n",
      "Accuracy on test set :  0.6589147286821705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alice\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "Y = df.loc[:,\"winner_home\"]\n",
    "X = df.loc[:,[c for c in df.columns if c!='winner_home']]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=0, stratify=Y)\n",
    "\n",
    "# select automatically the numerical and categorical columns\n",
    "num_col = X.select_dtypes([np.number]).columns\n",
    "cat_col = X.select_dtypes(\"object\").columns\n",
    "\n",
    "# Create pipeline for numeric features\n",
    "numeric_features = X.select_dtypes([np.number]).columns \n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')), # missing values in Age will be replaced by columns' mean\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Create pipeline for categorical features\n",
    "categorical_features = X.select_dtypes(\"object\").columns \n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), # missing values will be replaced by most frequent value\n",
    "    ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore')) # first column will be dropped to avoid creating correlations between features\n",
    "    ])\n",
    "\n",
    "# Use ColumnTranformer to make a preprocessor object that describes all the treatments to be done\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "Y_train_pred = model.predict(X_train)\n",
    "Y_test_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy on training set : \", accuracy_score(Y_train, Y_train_pred))\n",
    "print(\"Accuracy on test set : \", accuracy_score(Y_test, Y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model hypothesis: all the features, random state at 0, imputer = median or most frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set :  0.7232758620689655\n",
      "Accuracy on test set :  0.627906976744186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alice\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "Y = df.loc[:,\"winner_home\"]\n",
    "X = df.loc[:,[c for c in df.columns if c!='winner_home']]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=0, stratify=Y)\n",
    "\n",
    "# select automatically the numerical and categorical columns\n",
    "num_col = X.select_dtypes([np.number]).columns\n",
    "cat_col = X.select_dtypes(\"object\").columns\n",
    "\n",
    "# Create pipeline for numeric features\n",
    "numeric_features = X.select_dtypes([np.number]).columns \n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')), \n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Create pipeline for categorical features\n",
    "categorical_features = X.select_dtypes(\"object\").columns \n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant')), # missing values will be replaced by most frequent value\n",
    "    ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore')) # first column will be dropped to avoid creating correlations between features\n",
    "    ])\n",
    "\n",
    "# Use ColumnTranformer to make a preprocessor object that describes all the treatments to be done\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "Y_train_pred = model.predict(X_train)\n",
    "Y_test_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy on training set : \", accuracy_score(Y_train, Y_train_pred))\n",
    "print(\"Accuracy on test set : \", accuracy_score(Y_test, Y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model hypothesis: removal of the metric by team, random state at 0, imputer = mean or most frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set :  0.7129310344827586\n",
      "Accuracy on test set :  0.6434108527131783\n"
     ]
    }
   ],
   "source": [
    "Y = df2.loc[:,\"winner_home\"]\n",
    "X = df2.loc[:,[c for c in df2.columns if c!='winner_home']]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=0, stratify=Y)\n",
    "\n",
    "# select automatically the numerical and categorical columns\n",
    "num_col = X.select_dtypes([np.number]).columns\n",
    "cat_col = X.select_dtypes(\"object\").columns\n",
    "\n",
    "# Create pipeline for numeric features\n",
    "numeric_features = X.select_dtypes([np.number]).columns \n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')), # missing values in Age will be replaced by columns' mean\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Create pipeline for categorical features\n",
    "categorical_features = X.select_dtypes(\"object\").columns \n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), # missing values will be replaced by most frequent value\n",
    "    ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore')) # first column will be dropped to avoid creating correlations between features\n",
    "    ])\n",
    "\n",
    "# Use ColumnTranformer to make a preprocessor object that describes all the treatments to be done\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "Y_train_pred = model.predict(X_train)\n",
    "Y_test_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy on training set : \", accuracy_score(Y_train, Y_train_pred))\n",
    "print(\"Accuracy on test set : \", accuracy_score(Y_test, Y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ensemble methods\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "# import base estimators\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, ConfusionMatrixDisplay, RocCurveDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test with XGBoost regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df.loc[:,\"winner_home\"]\n",
    "X = df.loc[:,[c for c in df.columns if c!='winner_home']]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=0, stratify=Y)\n",
    "\n",
    "# select automatically the numerical and categorical columns\n",
    "num_col = X.select_dtypes([np.number]).columns\n",
    "cat_col = X.select_dtypes(\"object\").columns\n",
    "\n",
    "# Create pipeline for numeric features\n",
    "numeric_features = X.select_dtypes([np.number]).columns \n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')), # missing values in Age will be replaced by columns' mean\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Create pipeline for categorical features\n",
    "categorical_features = X.select_dtypes(\"object\").columns \n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), # missing values will be replaced by most frequent value\n",
    "    ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore')) # first column will be dropped to avoid creating correlations between features\n",
    "    ])\n",
    "\n",
    "# Use ColumnTranformer to make a preprocessor object that describes all the treatments to be done\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score XGBoost default train 0.9999991209005135\n",
      "\n",
      "\n",
      "score XGBoost default test -0.09899278737142359\n"
     ]
    }
   ],
   "source": [
    "regressor_xgb = XGBRegressor(max_depth=200)\n",
    "regressor_xgb.fit(X_train, Y_train)\n",
    "\n",
    "print(\"score XGBoost default train {}\".format(regressor_xgb.score(X_train, Y_train)))\n",
    "print(\"\\n\")\n",
    "print(\"score XGBoost default test {}\".format(regressor_xgb.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test with XGBoost classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test with all the features and the paramaters by default of XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df.loc[:,\"winner_home\"]\n",
    "X = df.loc[:,[c for c in df.columns if c!='winner_home']]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=0, stratify=Y)\n",
    "\n",
    "# select automatically the numerical and categorical columns\n",
    "num_col = X.select_dtypes([np.number]).columns\n",
    "cat_col = X.select_dtypes(\"object\").columns\n",
    "\n",
    "# Create pipeline for numeric features\n",
    "numeric_features = X.select_dtypes([np.number]).columns \n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')), # missing values in Age will be replaced by columns' mean\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Create pipeline for categorical features\n",
    "categorical_features = X.select_dtypes(\"object\").columns \n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), # missing values will be replaced by most frequent value\n",
    "    ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore')) # first column will be dropped to avoid creating correlations between features\n",
    "    ])\n",
    "\n",
    "# Use ColumnTranformer to make a preprocessor object that describes all the treatments to be done\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score XGBoost train 1.0\n",
      "\n",
      "\n",
      "score XGBoost test 0.6124031007751938\n"
     ]
    }
   ],
   "source": [
    "classifier_xgb = XGBClassifier(max_depth=200)\n",
    "classifier_xgb.fit(X_train, Y_train)\n",
    "\n",
    "print(\"score XGBoost train {}\".format(classifier_xgb.score(X_train, Y_train)))\n",
    "print(\"\\n\")\n",
    "print(\"score XGBoost test {}\".format(classifier_xgb.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test without the team metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df2.loc[:,\"winner_home\"]\n",
    "X = df2.loc[:,[c for c in df2.columns if c!='winner_home']]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=0, stratify=Y)\n",
    "\n",
    "# select automatically the numerical and categorical columns\n",
    "num_col = X.select_dtypes([np.number]).columns\n",
    "cat_col = X.select_dtypes(\"object\").columns\n",
    "\n",
    "# Create pipeline for numeric features\n",
    "numeric_features = X.select_dtypes([np.number]).columns \n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')), # missing values in Age will be replaced by columns' mean\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Create pipeline for categorical features\n",
    "categorical_features = X.select_dtypes(\"object\").columns \n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), # missing values will be replaced by most frequent value\n",
    "    ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore')) # first column will be dropped to avoid creating correlations between features\n",
    "    ])\n",
    "\n",
    "# Use ColumnTranformer to make a preprocessor object that describes all the treatments to be done\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score XGBoost train 1.0\n",
      "\n",
      "\n",
      "score XGBoost test 0.5736434108527132\n"
     ]
    }
   ],
   "source": [
    "classifier_xgb = XGBClassifier(max_depth=200)\n",
    "classifier_xgb.fit(X_train, Y_train)\n",
    "\n",
    "print(\"score XGBoost train {}\".format(classifier_xgb.score(X_train, Y_train)))\n",
    "print(\"\\n\")\n",
    "print(\"score XGBoost test {}\".format(classifier_xgb.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test with only the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['winner_home', 'pass_yds_MA_5_away', 'pass_td_MA_5_away',\n",
       "       'rush_yds_MA_5_away', 'rush_td_MA_5_away', 'fumbles_MA_5_away',\n",
       "       'fumbles_rec_MA_5_away', 'defense_sacks_MA_5_away',\n",
       "       'defense_td_MA_5_away', 'interceptions_MA_5_away',\n",
       "       'interceptions_td_MA_5_away', 'kicks_return_yds_MA_5_away',\n",
       "       'kicks_return_td_MA_5_away', 'punt_return_yds_MA_5_away',\n",
       "       'kicking_pts_MA_5_away', 'streak_home', 'pass_yds_MA_5_home',\n",
       "       'pass_td_MA_5_home', 'rush_yds_MA_5_home', 'rush_td_MA_5_home',\n",
       "       'rec_yds_MA_5_home', 'rec_td_MA_5_home', 'fumbles_MA_5_home',\n",
       "       'fumbles_rec_MA_5_home', 'defense_sacks_MA_5_home',\n",
       "       'defense_td_MA_5_home', 'interceptions_MA_5_home',\n",
       "       'interceptions_td_MA_5_home', 'kicks_return_yds_MA_5_home',\n",
       "       'kicks_return_td_MA_5_home', 'punt_return_yds_MA_5_home',\n",
       "       'kicking_pts_MA_5_home'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4 = df[team_metrics_win]\n",
    "df4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df4.loc[:,\"winner_home\"]\n",
    "X = df4.loc[:,[c for c in df4.columns if c!='winner_home']]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=0, stratify=Y)\n",
    "\n",
    "# select automatically the numerical and categorical columns\n",
    "num_col = X.select_dtypes([np.number]).columns\n",
    "cat_col = X.select_dtypes(\"object\").columns\n",
    "\n",
    "# Create pipeline for numeric features\n",
    "numeric_features = X.select_dtypes([np.number]).columns \n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')), # missing values in Age will be replaced by columns' mean\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Create pipeline for categorical features\n",
    "categorical_features = X.select_dtypes(\"object\").columns \n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), # missing values will be replaced by most frequent value\n",
    "    ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore')) # first column will be dropped to avoid creating correlations between features\n",
    "    ])\n",
    "\n",
    "# Use ColumnTranformer to make a preprocessor object that describes all the treatments to be done\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score XGBoost train 0.9706896551724138\n",
      "\n",
      "\n",
      "score XGBoost test 0.6201550387596899\n"
     ]
    }
   ],
   "source": [
    "classifier_xgb = XGBClassifier(max_depth=200)\n",
    "classifier_xgb.fit(X_train, Y_train)\n",
    "\n",
    "print(\"score XGBoost train {}\".format(classifier_xgb.score(X_train, Y_train)))\n",
    "print(\"\\n\")\n",
    "print(\"score XGBoost test {}\".format(classifier_xgb.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with team rating + win streak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_metrics_win2 = ['winner_home', 'pass_yds_MA_5_away', 'pass_td_MA_5_away', 'rush_yds_MA_5_away', 'rush_td_MA_5_away', 'fumbles_MA_5_away', 'fumbles_rec_MA_5_away', 'defense_sacks_MA_5_away', \n",
    "                'defense_td_MA_5_away', 'interceptions_MA_5_away', 'interceptions_td_MA_5_away', 'kicks_return_yds_MA_5_away', 'kicks_return_td_MA_5_away', 'punt_return_yds_MA_5_away', \n",
    "                'kicking_pts_MA_5_away', 'pass_yds_MA_5_home', 'pass_td_MA_5_home', 'rush_yds_MA_5_home', 'rush_td_MA_5_home', 'rec_yds_MA_5_home', 'rec_td_MA_5_home', \n",
    "                'fumbles_MA_5_home', 'fumbles_rec_MA_5_home', 'defense_sacks_MA_5_home', 'defense_td_MA_5_home', 'interceptions_MA_5_home', 'interceptions_td_MA_5_home', 'kicks_return_yds_MA_5_home', \n",
    "                'kicks_return_td_MA_5_home', 'punt_return_yds_MA_5_home', 'kicking_pts_MA_5_home', 'streak_away', 'streak_home']\n",
    "df5 = df[team_metrics_win2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df5.loc[:,\"winner_home\"]\n",
    "X = df5.loc[:,[c for c in df5.columns if c!='winner_home']]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=0, stratify=Y)\n",
    "\n",
    "# select automatically the numerical and categorical columns\n",
    "num_col = X.select_dtypes([np.number]).columns\n",
    "cat_col = X.select_dtypes(\"object\").columns\n",
    "\n",
    "# Create pipeline for numeric features\n",
    "numeric_features = X.select_dtypes([np.number]).columns \n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')), # missing values in Age will be replaced by columns' mean\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Create pipeline for categorical features\n",
    "categorical_features = X.select_dtypes(\"object\").columns \n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), # missing values will be replaced by most frequent value\n",
    "    ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore')) # first column will be dropped to avoid creating correlations between features\n",
    "    ])\n",
    "\n",
    "# Use ColumnTranformer to make a preprocessor object that describes all the treatments to be done\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score XGBoost train 0.978448275862069\n",
      "\n",
      "\n",
      "score XGBoost test 0.6201550387596899\n"
     ]
    }
   ],
   "source": [
    "classifier_xgb = XGBClassifier(max_depth=200)\n",
    "classifier_xgb.fit(X_train, Y_train)\n",
    "\n",
    "print(\"score XGBoost train {}\".format(classifier_xgb.score(X_train, Y_train)))\n",
    "print(\"\\n\")\n",
    "print(\"score XGBoost test {}\".format(classifier_xgb.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test with manually hand dataset: metrics, away team, home team, weather type, people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual = ['winner_home', 'pass_yds_MA_5_away', 'pass_td_MA_5_away', 'rush_yds_MA_5_away', 'rush_td_MA_5_away', 'fumbles_MA_5_away', 'fumbles_rec_MA_5_away', 'defense_sacks_MA_5_away', \n",
    "                'defense_td_MA_5_away', 'interceptions_MA_5_away', 'interceptions_td_MA_5_away', 'kicks_return_yds_MA_5_away', 'kicks_return_td_MA_5_away', 'punt_return_yds_MA_5_away', \n",
    "                'kicking_pts_MA_5_away','pass_yds_MA_5_home', 'pass_td_MA_5_home', 'rush_yds_MA_5_home', 'rush_td_MA_5_home', 'rec_yds_MA_5_home', 'rec_td_MA_5_home', \n",
    "                'fumbles_MA_5_home', 'fumbles_rec_MA_5_home', 'defense_sacks_MA_5_home', 'defense_td_MA_5_home', 'interceptions_MA_5_home', 'interceptions_td_MA_5_home', 'kicks_return_yds_MA_5_home', \n",
    "                'kicks_return_td_MA_5_home', 'punt_return_yds_MA_5_home', 'kicking_pts_MA_5_home', 'people', 'awayteam', 'hometeam', 'weather_type']\n",
    "df6 = df[manual]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df6.loc[:,\"winner_home\"]\n",
    "X = df6.loc[:,[c for c in df6.columns if c!='winner_home']]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=0, stratify=Y)\n",
    "\n",
    "# select automatically the numerical and categorical columns\n",
    "num_col = X.select_dtypes([np.number]).columns\n",
    "cat_col = X.select_dtypes(\"object\").columns\n",
    "\n",
    "# Create pipeline for numeric features\n",
    "numeric_features = X.select_dtypes([np.number]).columns \n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')), # missing values in Age will be replaced by columns' mean\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Create pipeline for categorical features\n",
    "categorical_features = X.select_dtypes(\"object\").columns \n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), # missing values will be replaced by most frequent value\n",
    "    ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore')) # first column will be dropped to avoid creating correlations between features\n",
    "    ])\n",
    "\n",
    "# Use ColumnTranformer to make a preprocessor object that describes all the treatments to be done\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score XGBoost train 0.9974137931034482\n",
      "\n",
      "\n",
      "score XGBoost test 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "classifier_xgb = XGBClassifier(max_depth=200)\n",
    "classifier_xgb.fit(X_train, Y_train)\n",
    "\n",
    "print(\"score XGBoost train {}\".format(classifier_xgb.score(X_train, Y_train)))\n",
    "print(\"\\n\")\n",
    "print(\"score XGBoost test {}\".format(classifier_xgb.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5698275862068964 0.048529199810474384\n"
     ]
    }
   ],
   "source": [
    "# trying the same model with the same parameter but with a kfold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(classifier_xgb, X_train, Y_train, cv=10)\n",
    "\n",
    "avg = scores.mean()\n",
    "std = scores.std()\n",
    "print(avg, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keep trying to improve the result on train by adding features: metrics, awayteam, hometeam, weather_type, people, temperature, stade, location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual2 = ['winner_home', 'pass_yds_MA_5_away', 'pass_td_MA_5_away', 'rush_yds_MA_5_away', 'rush_td_MA_5_away', 'fumbles_MA_5_away', 'fumbles_rec_MA_5_away', 'defense_sacks_MA_5_away', \n",
    "                'defense_td_MA_5_away', 'interceptions_MA_5_away', 'interceptions_td_MA_5_away', 'kicks_return_yds_MA_5_away', 'kicks_return_td_MA_5_away', 'punt_return_yds_MA_5_away', \n",
    "                'kicking_pts_MA_5_away', 'pass_yds_MA_5_home', 'pass_td_MA_5_home', 'rush_yds_MA_5_home', 'rush_td_MA_5_home', 'rec_yds_MA_5_home', 'rec_td_MA_5_home', \n",
    "                'fumbles_MA_5_home', 'fumbles_rec_MA_5_home', 'defense_sacks_MA_5_home', 'defense_td_MA_5_home', 'interceptions_MA_5_home', 'interceptions_td_MA_5_home', 'kicks_return_yds_MA_5_home', \n",
    "                'kicks_return_td_MA_5_home', 'punt_return_yds_MA_5_home', 'kicking_pts_MA_5_home','people', 'awayteam', 'hometeam', 'weather_type','delta_day_home', 'stade', 'location']\n",
    "df7 = df[manual2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score XGBoost train 1.0\n",
      "\n",
      "\n",
      "score XGBoost test 0.5658914728682171\n"
     ]
    }
   ],
   "source": [
    "Y = df7.loc[:,\"winner_home\"]\n",
    "X = df7.loc[:,[c for c in df7.columns if c!='winner_home']]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=0, stratify=Y)\n",
    "\n",
    "# select automatically the numerical and categorical columns\n",
    "num_col = X.select_dtypes([np.number]).columns\n",
    "cat_col = X.select_dtypes(\"object\").columns\n",
    "\n",
    "# Create pipeline for numeric features\n",
    "numeric_features = X.select_dtypes([np.number]).columns \n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')), # missing values in Age will be replaced by columns' mean\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Create pipeline for categorical features\n",
    "categorical_features = X.select_dtypes(\"object\").columns \n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), # missing values will be replaced by most frequent value\n",
    "    ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore')) # first column will be dropped to avoid creating correlations between features\n",
    "    ])\n",
    "\n",
    "# Use ColumnTranformer to make a preprocessor object that describes all the treatments to be done\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "classifier_xgb = XGBClassifier(max_depth=200)\n",
    "classifier_xgb.fit(X_train, Y_train)\n",
    "\n",
    "print(\"score XGBoost train {}\".format(classifier_xgb.score(X_train, Y_train)))\n",
    "print(\"\\n\")\n",
    "print(\"score XGBoost test {}\".format(classifier_xgb.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df.loc[:,\"winner_home\"]\n",
    "X = df.loc[:,[c for c in df.columns if c!='winner_home']]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=0, stratify=Y)\n",
    "\n",
    "# select automatically the numerical and categorical columns\n",
    "num_col = X.select_dtypes([np.number]).columns\n",
    "cat_col = X.select_dtypes(\"object\").columns\n",
    "\n",
    "# Create pipeline for numeric features\n",
    "numeric_features = X.select_dtypes([np.number]).columns \n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')), # missing values in Age will be replaced by columns' mean\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Create pipeline for categorical features\n",
    "categorical_features = X.select_dtypes(\"object\").columns \n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), # missing values will be replaced by most frequent value\n",
    "    ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore')) # first column will be dropped to avoid creating correlations between features\n",
    "    ])\n",
    "\n",
    "# Use ColumnTranformer to make a preprocessor object that describes all the treatments to be done\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.627906976744186"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_train, Y_train)\n",
    "svm.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "{'C': 1, 'gamma': 0.0001}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.627906976744186"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying to improve the first score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [1, 5, 10, 50],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005]}\n",
    "\n",
    "grid = GridSearchCV(svm, param_grid, verbose=2, n_jobs=-1)\n",
    "grid.fit(X_train, Y_train)\n",
    "print(grid.best_params_)\n",
    "model = grid.best_estimator_\n",
    "model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df2.loc[:,\"winner_home\"]\n",
    "X = df2.loc[:,[c for c in df2.columns if c!='winner_home']]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=0, stratify=Y)\n",
    "\n",
    "# select automatically the numerical and categorical columns\n",
    "num_col = X.select_dtypes([np.number]).columns\n",
    "cat_col = X.select_dtypes(\"object\").columns\n",
    "\n",
    "# Create pipeline for numeric features\n",
    "numeric_features = X.select_dtypes([np.number]).columns \n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')), # missing values in Age will be replaced by columns' mean\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Create pipeline for categorical features\n",
    "categorical_features = X.select_dtypes(\"object\").columns \n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), # missing values will be replaced by most frequent value\n",
    "    ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore')) # first column will be dropped to avoid creating correlations between features\n",
    "    ])\n",
    "\n",
    "# Use ColumnTranformer to make a preprocessor object that describes all the treatments to be done\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6124031007751938"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_train, Y_train)\n",
    "svm.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alice\\OneDrive\\Bureau\\Jedha\\Game_predictor_NFL\\nfl\\03_modelisation\\modelisation_final_dataset\\models_ab_v2.ipynb Cellule 48\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alice/OneDrive/Bureau/Jedha/Game_predictor_NFL/nfl/03_modelisation/modelisation_final_dataset/models_ab_v2.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m param_grid \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m1\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m50\u001b[39m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alice/OneDrive/Bureau/Jedha/Game_predictor_NFL/nfl/03_modelisation/modelisation_final_dataset/models_ab_v2.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m               \u001b[39m'\u001b[39m\u001b[39mgamma\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m0.0001\u001b[39m, \u001b[39m0.0005\u001b[39m, \u001b[39m0.001\u001b[39m, \u001b[39m0.005\u001b[39m]}\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alice/OneDrive/Bureau/Jedha/Game_predictor_NFL/nfl/03_modelisation/modelisation_final_dataset/models_ab_v2.ipynb#X41sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m grid \u001b[39m=\u001b[39m GridSearchCV(svm, param_grid, verbose\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/alice/OneDrive/Bureau/Jedha/Game_predictor_NFL/nfl/03_modelisation/modelisation_final_dataset/models_ab_v2.ipynb#X41sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m grid\u001b[39m.\u001b[39;49mfit(X_train, Y_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alice/OneDrive/Bureau/Jedha/Game_predictor_NFL/nfl/03_modelisation/modelisation_final_dataset/models_ab_v2.ipynb#X41sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(grid\u001b[39m.\u001b[39mbest_params_)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alice/OneDrive/Bureau/Jedha/Game_predictor_NFL/nfl/03_modelisation/modelisation_final_dataset/models_ab_v2.ipynb#X41sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model \u001b[39m=\u001b[39m grid\u001b[39m.\u001b[39mbest_estimator_\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    869\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    871\u001b[0m     )\n\u001b[0;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 875\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    877\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    879\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:1379\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1378\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1379\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    815\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    817\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    819\u001b[0m         )\n\u001b[0;32m    820\u001b[0m     )\n\u001b[1;32m--> 822\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    823\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    824\u001b[0m         clone(base_estimator),\n\u001b[0;32m    825\u001b[0m         X,\n\u001b[0;32m    826\u001b[0m         y,\n\u001b[0;32m    827\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    828\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    829\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    830\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    831\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    832\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    833\u001b[0m     )\n\u001b[0;32m    834\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    835\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    836\u001b[0m     )\n\u001b[0;32m    837\u001b[0m )\n\u001b[0;32m    839\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    840\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    844\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1056\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[0;32m   1057\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1058\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    933\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    934\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 935\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[0;32m    936\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    937\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    540\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 542\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m    543\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    544\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.1776.0_x64__qbz5n2kfra8p0\\lib\\concurrent\\futures\\_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[1;32m--> 453\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.1776.0_x64__qbz5n2kfra8p0\\lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[0;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# trying to improve the first score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [1, 5, 10, 50],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005]}\n",
    "\n",
    "grid = GridSearchCV(svm, param_grid, verbose=2, n_jobs=-1)\n",
    "grid.fit(X_train, Y_train)\n",
    "print(grid.best_params_)\n",
    "model = grid.best_estimator_\n",
    "model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model hypothesis: removal of the coach and the metrics by team, random state at 0, imputer = mean or most frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['season', 'week', 'awayteam', 'hometeam', 'winner_home', 'weather_type', 'temperature', 'humidity', 'wind', 'streak_away', 'delta_day_away', 'delta_day_home', 'stade', 'location', 'people', 'month', 'dayofmonth', 'dayofweek']\n"
     ]
    }
   ],
   "source": [
    "df3 = df2.drop(columns=['home_coach', 'away_coach', 'idgame'], axis=1)\n",
    "print(df3.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df3.loc[:,\"winner_home\"]\n",
    "X = df3.loc[:,[c for c in df3.columns if c!='winner_home']]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=0, stratify=Y)\n",
    "\n",
    "# select automatically the numerical and categorical columns\n",
    "num_col = X.select_dtypes([np.number]).columns\n",
    "cat_col = X.select_dtypes(\"object\").columns\n",
    "\n",
    "# Create pipeline for numeric features\n",
    "numeric_features = X.select_dtypes([np.number]).columns \n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')), # missing values in Age will be replaced by columns' mean\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Create pipeline for categorical features\n",
    "categorical_features = X.select_dtypes(\"object\").columns \n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), # missing values will be replaced by most frequent value\n",
    "    ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore')) # first column will be dropped to avoid creating correlations between features\n",
    "    ])\n",
    "\n",
    "# Use ColumnTranformer to make a preprocessor object that describes all the treatments to be done\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.627906976744186"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_train, Y_train)\n",
    "svm.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "{'C': 10, 'gamma': 0.0001}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6124031007751938"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying to improve the first score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [1, 5, 10, 50],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005]}\n",
    "\n",
    "grid = GridSearchCV(svm, param_grid, verbose=2, n_jobs=-1)\n",
    "grid.fit(X_train, Y_train)\n",
    "print(grid.best_params_)\n",
    "model = grid.best_estimator_\n",
    "model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0f83c99e3f58fcb45df72ba14dd4c5dca51a52ef6a1364f65bf6ca14e4a1196"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
